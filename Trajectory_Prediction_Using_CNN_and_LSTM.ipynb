{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from glob import glob\n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sort import Sort\n",
    "from torch.amp import autocast\n",
    "from torch.amp import GradScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the chunksize to avoid OverflowError in matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['agg.path.chunksize'] = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Detections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detections(images_dir, labels_dir):\n",
    "    image_files = sorted(os.listdir(images_dir))\n",
    "    detections = []\n",
    "    img_paths = []\n",
    "\n",
    "    for idx, img_file in enumerate(image_files):\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        img_name, _ = os.path.splitext(img_file)\n",
    "\n",
    "        # Corresponding label file\n",
    "        lbl_path = os.path.join(labels_dir, img_name + '.txt')\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        width, height = img.size\n",
    "\n",
    "        # Read label file\n",
    "        frame_detections = []\n",
    "        if os.path.exists(lbl_path):\n",
    "            with open(lbl_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tokens = line.strip().split()\n",
    "                if len(tokens) != 5:\n",
    "                    continue\n",
    "                class_id = int(tokens[0])  # Class ID\n",
    "                x_center = float(tokens[1]) * width\n",
    "                y_center = float(tokens[2]) * height\n",
    "                bbox_width = float(tokens[3]) * width\n",
    "                bbox_height = float(tokens[4]) * height\n",
    "\n",
    "                x1 = x_center - bbox_width / 2\n",
    "                y1 = y_center - bbox_height / 2\n",
    "                x2 = x_center + bbox_width / 2\n",
    "                y2 = y_center + bbox_height / 2\n",
    "\n",
    "                # The detection format for SORT is [x1, y1, x2, y2, score]\n",
    "                frame_detections.append([x1, y1, x2, y2, 1.0])  # Assume a confidence score of 1.0\n",
    "        else:\n",
    "            frame_detections = np.empty((0, 5))\n",
    "\n",
    "        detections.append(np.array(frame_detections))\n",
    "        img_paths.append(img_path)\n",
    "    return detections, img_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply SORT to Assign IDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_objects(detections):\n",
    "    mot_tracker = Sort()  # Create instance of the SORT tracker\n",
    "    trajectories = {}\n",
    "    frame_indices = []\n",
    "\n",
    "    for frame_idx, dets in enumerate(detections):\n",
    "        tracks = mot_tracker.update(dets)\n",
    "        for track in tracks:\n",
    "            x1, y1, x2, y2, track_id = track\n",
    "            center_x = (x1 + x2) / 2\n",
    "            center_y = (y1 + y2) / 2\n",
    "            if track_id not in trajectories:\n",
    "                trajectories[track_id] = []\n",
    "            trajectories[track_id].append({\n",
    "                'frame_idx': frame_idx,\n",
    "                'bbox': [x1, y1, x2, y2],\n",
    "                'center': [center_x, center_y]\n",
    "            })\n",
    "    return trajectories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect Trajectories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectories(images_dir, labels_dir):\n",
    "    detections, img_paths = load_detections(images_dir, labels_dir)\n",
    "    trajectories = track_objects(detections)\n",
    "    return trajectories, img_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Paths to images and labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_dir = './DETRAC_Upload/images/train'\n",
    "train_labels_dir = './DETRAC_Upload/labels/train'\n",
    "val_images_dir = './DETRAC_Upload/images/val'\n",
    "val_labels_dir = './DETRAC_Upload/labels/val'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get trajectories for training and validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trajectories, train_image_files = get_trajectories(train_images_dir, train_labels_dir)\n",
    "val_trajectories, val_image_files = get_trajectories(val_images_dir, val_labels_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Simple Trajectory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points in trajectory: 157\n"
     ]
    }
   ],
   "source": [
    "if len(train_trajectories) > 0:\n",
    "    sample_track_id = list(train_trajectories.keys())[0]\n",
    "    sample_trajectory = train_trajectories[sample_track_id]\n",
    "\n",
    "    positions = np.array([frame_info['center'] for frame_info in sample_trajectory])\n",
    "\n",
    "    # Print the number of points\n",
    "    print(f\"Number of points in trajectory: {len(positions)}\")\n",
    "\n",
    "    # Limit the number of points to plot if necessary\n",
    "    max_points = 1000  # Adjust this value as needed\n",
    "    if len(positions) > max_points:\n",
    "        positions = positions[:max_points]\n",
    "        print(f\"Trajectory is too long; plotting first {max_points} points.\")\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(positions[:, 0], positions[:, 1], marker='o')\n",
    "    plt.title('Sample Vehicle Trajectory')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No trajectories found in the training data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Dataset Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UADETRACDataset(Dataset):\n",
    "    def __init__(self, images_dir, trajectories, image_paths, input_length, pred_length, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        self.input_length = input_length\n",
    "        self.pred_length = pred_length\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        self.create_sequences(trajectories)\n",
    "\n",
    "    def create_sequences(self, trajectories):\n",
    "        for track_id, detections in trajectories.items():\n",
    "            # Sort detections by frame_idx\n",
    "            detections = sorted(detections, key=lambda x: x['frame_idx'])\n",
    "            num_frames = len(detections)\n",
    "            if num_frames < self.input_length + self.pred_length:\n",
    "                continue\n",
    "            for i in range(num_frames - self.input_length - self.pred_length + 1):\n",
    "                input_frames = detections[i:i+self.input_length]\n",
    "                target_frames = detections[i+self.input_length:i+self.input_length+self.pred_length]\n",
    "                self.sequences.append((input_frames, target_frames))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_frames, target_frames = self.sequences[idx]\n",
    "\n",
    "        img_seq = []\n",
    "        pos_seq = []\n",
    "        for frame_info in input_frames:\n",
    "            frame_idx = frame_info['frame_idx']\n",
    "            img_path = self.image_paths[frame_idx]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "            img_seq.append(img)\n",
    "            pos_seq.append(frame_info['center'])\n",
    "\n",
    "        target_pos_seq = [frame_info['center'] for frame_info in target_frames]\n",
    "\n",
    "        img_seq = torch.stack(img_seq)  # Shape: (input_length, C, H, W)\n",
    "        pos_seq = torch.tensor(pos_seq).float()  # Shape: (input_length, 2)\n",
    "        target_seq = torch.tensor(target_pos_seq).float()  # Shape: (pred_length, 2)\n",
    "\n",
    "        # Normalize positions by image dimensions if desired\n",
    "        # Assuming all images have the same dimensions\n",
    "        # img_width, img_height = img.size\n",
    "        # pos_seq /= torch.tensor([img_width, img_height])\n",
    "        # target_seq /= torch.tensor([img_width, img_height])\n",
    "\n",
    "        return img_seq, pos_seq, target_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet means\n",
    "                         std=[0.229, 0.224, 0.225])    # ImageNet stds\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Datasets and DataLoaders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 10\n",
    "pred_length = 5\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = UADETRACDataset(train_images_dir, train_trajectories, train_image_files, input_length, pred_length, transform=transform)\n",
    "val_dataset = UADETRACDataset(val_images_dir, val_trajectories, val_image_files, input_length, pred_length, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Feature Extractor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a pre-trained ResNet18 model for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        # Use a pre-trained ResNet50 model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the last fully connected layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, C, H, W)\n",
    "        x = self.resnet(x)  # Output shape: (batch_size, 512, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # Shape: (batch_size, 512)\n",
    "        x = self.fc(x)  # Shape: (batch_size, 128)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Trajectory Predictor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the LSTM Model for trajectory prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryPredictor(nn.Module):\n",
    "    def __init__(self, feature_dim=128, hidden_size=256, num_layers=1, pred_length=pred_length):\n",
    "        super(TrajectoryPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=feature_dim + 2, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, pred_length * 2)\n",
    "        \n",
    "    def forward(self, features, positions):\n",
    "        # features: (batch_size, seq_length, feature_dim)\n",
    "        # positions: (batch_size, seq_length, 2)\n",
    "        x = torch.cat([features, positions], dim=2)  # Shape: (batch_size, seq_length, feature_dim + 2)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Get the last output\n",
    "        out = self.fc(out)   # Shape: (batch_size, pred_length * 2)\n",
    "        out = out.view(-1, pred_length, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleTrajectoryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VehicleTrajectoryModel, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.predictor = TrajectoryPredictor()\n",
    "        \n",
    "    def forward(self, img_seq, pos_seq):\n",
    "        batch_size, seq_length, C, H, W = img_seq.size()\n",
    "        img_seq = img_seq.view(batch_size * seq_length, C, H, W)\n",
    "        features = self.feature_extractor(img_seq)\n",
    "        features = features.view(batch_size, seq_length, -1)\n",
    "        out = self.predictor(features, pos_seq)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model into GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amamo\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\amamo\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = VehicleTrajectoryModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VehicleTrajectoryModel(\n",
      "  (feature_extractor): FeatureExtractor(\n",
      "    (resnet): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "    (fc): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  )\n",
      "  (predictor): TrajectoryPredictor(\n",
      "    (lstm): LSTM(130, 256, batch_first=True)\n",
      "    (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler('cuda')\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 47715.6865\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for img_seq, pos_seq, target_seq in train_loader:\n",
    "        img_seq = img_seq.to(device, non_blocking=True)\n",
    "        pos_seq = pos_seq.to(device, non_blocking=True)\n",
    "        target_seq = target_seq.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(img_seq, pos_seq)\n",
    "            loss = criterion(outputs, target_seq)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for img_seq, pos_seq, target_seq in val_loader:\n",
    "        img_seq = img_seq.to(device)\n",
    "        pos_seq = pos_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "        \n",
    "        outputs = model(img_seq, pos_seq)\n",
    "        loss = criterion(outputs, target_seq)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "test_loss /= len(val_loader)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize Model Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test data\n",
    "img_seq, pos_seq, target_seq = next(iter(val_loader))\n",
    "img_seq = img_seq.to(device)\n",
    "pos_seq = pos_seq.to(device)\n",
    "outputs = model(img_seq, pos_seq)\n",
    "\n",
    "img_seq = img_seq.cpu()\n",
    "pos_seq = pos_seq.cpu()\n",
    "outputs = outputs.cpu()\n",
    "target_seq = target_seq.cpu()\n",
    "\n",
    "# Plotting\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    input_positions = pos_seq[i].numpy()\n",
    "    true_positions = np.concatenate((input_positions, target_seq[i].numpy()))\n",
    "    pred_positions = np.concatenate((input_positions, outputs[i].detach().numpy()))\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(true_positions[:, 0], true_positions[:, 1], 'bo-', label='True Trajectory')\n",
    "    plt.plot(pred_positions[:, 0], pred_positions[:, 1], 'ro--', label='Predicted Trajectory')\n",
    "    plt.legend()\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.title(f'Sample {i+1}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
